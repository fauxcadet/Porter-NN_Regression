{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1FyiW_ehfu-",
        "outputId": "2dda9a84-fd30-48d5-c80b-4f86c042e494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2708911147.py:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['store_avg_delivery_time'].fillna(global_mean, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tools for Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Tools for Neural Network and Tuning\n",
        "import tensorflow as tf # <-- CRITICAL: ADD THIS LINE for callbacks!\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. Data Loading and Initial Cleaning ---\n",
        "# Load dataset, parsing datetime columns\n",
        "df = pd.read_csv(\"dataset.csv\", parse_dates=[1, 2])\n",
        "# Renaming column names by index is dangerous; rely on columns 1 and 2 being datetimes.\n",
        "# Keeping 'store_id' for feature engineering before dropping later.\n",
        "\n",
        "# --- 2. Feature Engineering ---\n",
        "# Create the raw time difference (timedelta object)\n",
        "df['time_taken'] = df['actual_delivery_time'] - df['created_at']\n",
        "\n",
        "# Convert the timedelta object to the numerical target variable (minutes)\n",
        "df['time_taken_mins'] = df['time_taken'].dt.total_seconds() / 60\n",
        "\n",
        "# Extract time-based features from the creation time\n",
        "df['hours'] = df['created_at'].dt.hour\n",
        "df['day'] = df['created_at'].dt.dayofweek # Monday=0, Sunday=6\n",
        "\n",
        "\n",
        "# --- NEW: Historical Average Delivery Time per Store ---\n",
        "# Calculate the mean delivery time for each store and merge it back\n",
        "store_avg_time = df.groupby('store_id')['time_taken_mins'].mean().reset_index()\n",
        "store_avg_time.rename(columns={'time_taken_mins': 'store_avg_delivery_time'}, inplace=True)\n",
        "df = pd.merge(df, store_avg_time, on='store_id', how='left')\n",
        "\n",
        "# Fill NaNs (if any new store IDs) with the global mean\n",
        "global_mean = df['store_avg_delivery_time'].mean()\n",
        "df['store_avg_delivery_time'].fillna(global_mean, inplace=True)\n",
        "\n",
        "\n",
        "# --- NEW: Temporal Features (Rush Hour Flags) ---\n",
        "# 1. Lunch Rush (11:00 to 14:00 inclusive)\n",
        "df['is_lunch_rush'] = df['hours'].apply(lambda x: 1 if 11 <= x <= 14 else 0)\n",
        "# 2. Dinner Rush (17:00 to 21:00 inclusive)\n",
        "df['is_dinner_rush'] = df['hours'].apply(lambda x: 1 if 17 <= x <= 21 else 0)\n",
        "# 3. Weekend Flag (5=Sat, 6=Sun)\n",
        "df['is_weekend'] = df['day'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "\n",
        "# --- 3. Final Cleaning and Encoding ---\n",
        "# Drop the original datetime columns, the raw timedelta, and the store_id (now that we have its avg time)\n",
        "df.drop(['time_taken', 'created_at', 'actual_delivery_time', 'store_id'], axis=1, inplace=True)\n",
        "\n",
        "# Remove rows with any missing data\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Use One-Hot Encoding for the categorical feature (preferred for non-ordinal features)\n",
        "df = pd.get_dummies(df, columns=['store_primary_category'], prefix='category', dummy_na=False)\n",
        "\n",
        "# --- 4. Prepare Data for Modeling ---\n",
        "y = df['time_taken_mins']\n",
        "x = df.drop(['time_taken_mins'], axis=1)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to run this command in your environment once\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2a2F6GhxfA",
        "outputId": "12fc73dd-2a8c-4564-c100-6edddb377ae9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.12/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (2.32.4)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.12/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras->keras-tuner) (0.5.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->keras-tuner) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras->keras-tuner) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras->keras-tuner) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# --- 5. Scaling Data for NN ---\n",
        "# Initialize and fit the scaler ONLY on training data\n",
        "scaler = MinMaxScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "# --- 6. Define the Hypermodel Function for Keras Tuner ---\n",
        "def build_model(hp):\n",
        "    # Get the number of input features\n",
        "    input_dim = x_train_scaled.shape[1]\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input Layer (Fixed)\n",
        "    model.add(Dense(input_dim, kernel_initializer='normal', activation='relu', input_shape=(input_dim,)))\n",
        "\n",
        "    # Hyperparameter for the number of hidden layers (1 to 3)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        # Hyperparameter for the size of each dense layer\n",
        "        model.add(Dense(\n",
        "            # Units range from 32 to 256, stepping by 32\n",
        "            units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
        "            activation='relu'\n",
        "        ))\n",
        "\n",
        "    # Output Layer (Fixed - use 'linear' for regression)\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Hyperparameter for the learning rate\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    # Compile the model\n",
        "    adam = Adam(learning_rate=hp_learning_rate)\n",
        "    model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# --- 7. Run the Tuning Process ---\n",
        "print(\"Starting Hyperparameter Tuning...\")\n",
        "\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_mae',    # Minimize Mean Absolute Error on validation data\n",
        "    max_trials=10,          # Try 10 different combinations\n",
        "    executions_per_trial=1, # One run per combination for speed\n",
        "    directory='keras_tuning',\n",
        "    project_name='delivery_time_nn_v2'\n",
        ")\n",
        "\n",
        "# Stop training early if validation MAE stops improving for 5 epochs\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=5)\n",
        "\n",
        "# Start the search\n",
        "# We use a validation split of 20% to guide the tuner\n",
        "tuner.search(\n",
        "    x_train_scaled,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    validation_split=0.2,\n",
        "    batch_size=128,\n",
        "    callbacks=[stop_early],\n",
        "    verbose=0 # Set verbose to 1 to see more logs\n",
        ")\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(\"\\n--- Hyperparameter Tuning Complete ---\")\n",
        "print(f\"Best Learning Rate: {best_hps.get('learning_rate')}\")\n",
        "print(f\"Best Number of Layers: {best_hps.get('num_layers')}\")\n",
        "# You can print more best_hps here\n",
        "print(\"Found best model.\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "NTLr-Vy1hoPO",
        "outputId": "ac4a0fe7-7151-4f57-94d7-0c7dcb3deb59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# --- 5. Scaling Data for NN ---\\n# Initialize and fit the scaler ONLY on training data\\nscaler = MinMaxScaler()\\nx_train_scaled = scaler.fit_transform(x_train)\\nx_test_scaled = scaler.transform(x_test)\\n\\n\\n# --- 6. Define the Hypermodel Function for Keras Tuner ---\\ndef build_model(hp):\\n    # Get the number of input features\\n    input_dim = x_train_scaled.shape[1] \\n    \\n    model = Sequential()\\n    \\n    # Input Layer (Fixed)\\n    model.add(Dense(input_dim, kernel_initializer=\\'normal\\', activation=\\'relu\\', input_shape=(input_dim,)))\\n\\n    # Hyperparameter for the number of hidden layers (1 to 3)\\n    for i in range(hp.Int(\\'num_layers\\', 1, 3)):\\n        # Hyperparameter for the size of each dense layer\\n        model.add(Dense(\\n            # Units range from 32 to 256, stepping by 32\\n            units=hp.Int(f\\'units_{i}\\', min_value=32, max_value=256, step=32),\\n            activation=\\'relu\\'\\n        ))\\n        \\n    # Output Layer (Fixed - use \\'linear\\' for regression)\\n    model.add(Dense(1, activation=\\'linear\\'))\\n\\n    # Hyperparameter for the learning rate\\n    hp_learning_rate = hp.Choice(\\'learning_rate\\', values=[1e-2, 1e-3, 1e-4])\\n\\n    # Compile the model\\n    adam = Adam(learning_rate=hp_learning_rate)\\n    model.compile(optimizer=adam, loss=\\'mse\\', metrics=[\\'mae\\'])\\n    \\n    return model\\n\\n\\n# --- 7. Run the Tuning Process ---\\nprint(\"Starting Hyperparameter Tuning...\")\\n\\ntuner = kt.RandomSearch(\\n    build_model,            \\n    objective=\\'val_mae\\',    # Minimize Mean Absolute Error on validation data\\n    max_trials=10,          # Try 10 different combinations\\n    executions_per_trial=1, # One run per combination for speed\\n    directory=\\'keras_tuning\\', \\n    project_name=\\'delivery_time_nn_v2\\'\\n)\\n\\n# Stop training early if validation MAE stops improving for 5 epochs\\nstop_early = tf.keras.callbacks.EarlyStopping(monitor=\\'val_mae\\', patience=5)\\n\\n# Start the search\\n# We use a validation split of 20% to guide the tuner\\ntuner.search(\\n    x_train_scaled, \\n    y_train, \\n    epochs=50, \\n    validation_split=0.2,\\n    batch_size=128,\\n    callbacks=[stop_early],\\n    verbose=0 # Set verbose to 1 to see more logs\\n)\\n\\n# Get the best model\\nbest_model = tuner.get_best_models(num_models=1)[0]\\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\\n\\nprint(\"\\n--- Hyperparameter Tuning Complete ---\")\\nprint(f\"Best Learning Rate: {best_hps.get(\\'learning_rate\\')}\")\\nprint(f\"Best Number of Layers: {best_hps.get(\\'num_layers\\')}\")\\n# You can print more best_hps here\\nprint(\"Found best model.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Final Training of the Best Model ---\n",
        "# Retrain the best model with more epochs for the final run\n",
        "print(\"\\nRetraining final best model for 100 epochs...\")\n",
        "final_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Use a longer EarlyStopping patience for final training\n",
        "stop_final = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
        "\n",
        "history_final = final_model.fit(\n",
        "    x_train_scaled,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    batch_size=128,\n",
        "    callbacks=[stop_final],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- 9. Final Evaluation ---\n",
        "# 1. Get Prediction\n",
        "nn_prediction = final_model.predict(x_test_scaled)\n",
        "\n",
        "# CRITICAL FIX: FLATTEN THE PREDICTIONS\n",
        "nn_prediction = nn_prediction.flatten()\n",
        "\n",
        "# 2. Define MAPE function\n",
        "def MAPE(y_actual, y_predicted):\n",
        "    epsilon = 1e-10\n",
        "    y_actual = np.asarray(y_actual)\n",
        "    mape = np.mean(np.abs((y_actual - y_predicted) / (y_actual + epsilon))) * 100\n",
        "    return mape\n",
        "\n",
        "# 3. Calculate all metrics\n",
        "nn_mse = mean_squared_error(y_test, nn_prediction)\n",
        "nn_rmse = nn_mse ** 0.5\n",
        "nn_mae = mean_absolute_error(y_test, nn_prediction)\n",
        "nn_r2 = r2_score(y_test, nn_prediction)\n",
        "nn_mape = MAPE(y_test, nn_prediction)\n",
        "\n",
        "# 4. Print the final results\n",
        "print(\"\\n--- FINAL NEURAL NETWORK TEST PERFORMANCE (Tuned & Enhanced Features) ---\")\n",
        "print(f'Mean Squared Error (MSE): {nn_mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {nn_rmse:.2f} minutes')\n",
        "print(f'Mean Absolute Error (MAE): {nn_mae:.2f} minutes')\n",
        "print(f'R-squared (R2 Score): {nn_r2:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {nn_mape:.2f}%')\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "mj2N9r84iHj0",
        "outputId": "21766b63-805a-48b6-a43b-64318f6a4066"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retraining final best model for 100 epochs...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tuner' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11604325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Retrain the best model with more epochs for the final run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRetraining final best model for 100 epochs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfinal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Use a longer EarlyStopping patience for final training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "after gettinhg new parameter\n",
        "new"
      ],
      "metadata": {
        "id": "8QkJ3YRylaST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# --- 1. Data Loading and Feature Engineering ---\n",
        "\n",
        "# Load dataset, parsing datetime columns\n",
        "df = pd.read_csv(\"dataset.csv\", parse_dates=[1, 2])\n",
        "\n",
        "# Create the target variable (delivery time in minutes)\n",
        "df['time_taken'] = df['actual_delivery_time'] - df['created_at']\n",
        "df['time_taken_mins'] = df['time_taken'].dt.total_seconds() / 60\n",
        "\n",
        "# Extract time-based features\n",
        "df['hours'] = df['created_at'].dt.hour\n",
        "df['day'] = df['created_at'].dt.dayofweek # Monday=0, Sunday=6\n",
        "\n",
        "\n",
        "# --- 2. Advanced Feature Engineering ---\n",
        "\n",
        "# Historical Average Delivery Time per Store\n",
        "store_avg_time = df.groupby('store_id')['time_taken_mins'].mean().reset_index()\n",
        "store_avg_time.rename(columns={'time_taken_mins': 'store_avg_delivery_time'}, inplace=True)\n",
        "df = pd.merge(df, store_avg_time, on='store_id', how='left')\n",
        "global_mean = df['store_avg_delivery_time'].mean()\n",
        "df['store_avg_delivery_time'].fillna(global_mean, inplace=True)\n",
        "\n",
        "# Temporal Features (Rush Hour Flags)\n",
        "df['is_lunch_rush'] = df['hours'].apply(lambda x: 1 if 11 <= x <= 14 else 0)\n",
        "df['is_dinner_rush'] = df['hours'].apply(lambda x: 1 if 17 <= x <= 21 else 0)\n",
        "df['is_weekend'] = df['day'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "\n",
        "# --- 3. Final Cleaning and Encoding ---\n",
        "\n",
        "# Drop original columns and handle NaNs\n",
        "df.drop(['time_taken', 'created_at', 'actual_delivery_time', 'store_id'], axis=1, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# One-Hot Encoding for the categorical feature\n",
        "df = pd.get_dummies(df, columns=['store_primary_category'], prefix='category', dummy_na=False)\n",
        "\n",
        "\n",
        "# --- 4. Prepare Data for Modeling ---\n",
        "\n",
        "y = df['time_taken_mins']\n",
        "x = df.drop(['time_taken_mins'], axis=1)\n",
        "\n",
        "# Split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling Data for NN\n",
        "scaler = MinMaxScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "\n",
        "# --- 5. Build Final Tuned Model ---\n",
        "\n",
        "input_dim = x_train_scaled.shape[1]\n",
        "FINAL_LEARNING_RATE = 0.01 # Best Hyperparameter\n",
        "FINAL_NUM_LAYERS = 2       # Best Hyperparameter\n",
        "\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(input_dim, kernel_initializer='normal', activation='relu', input_shape=(input_dim,)))\n",
        "\n",
        "# Build 2 hidden layers (as per Best Hyperparameters)\n",
        "# We will use the average layer size from the search (128 units is a good choice)\n",
        "final_model.add(Dense(128, activation='relu'))\n",
        "final_model.add(Dense(64, activation='relu')) # Second layer\n",
        "\n",
        "# Output Layer\n",
        "final_model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with the best learning rate\n",
        "adam = Adam(learning_rate=FINAL_LEARNING_RATE)\n",
        "final_model.compile(optimizer=adam, loss='mse', metrics=['mae'])\n",
        "\n",
        "\n",
        "# --- 6. Final Training and Evaluation ---\n",
        "\n",
        "print(\"\\nRetraining final tuned model...\")\n",
        "\n",
        "# Use EarlyStopping to prevent overfitting during final training\n",
        "stop_final = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=10, restore_best_weights=True)\n",
        "\n",
        "history_final = final_model.fit(\n",
        "    x_train_scaled,\n",
        "    y_train,\n",
        "    epochs=100,\n",
        "    validation_split=0.2,\n",
        "    batch_size=128,\n",
        "    callbacks=[stop_final],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 1. Get Prediction\n",
        "nn_prediction = final_model.predict(x_test_scaled)\n",
        "nn_prediction = nn_prediction.flatten() # CRITICAL FIX: Flatten the output\n",
        "\n",
        "# 2. Define MAPE function\n",
        "def MAPE(y_actual, y_predicted):\n",
        "    epsilon = 1e-10\n",
        "    y_actual = np.asarray(y_actual)\n",
        "    mape = np.mean(np.abs((y_actual - y_predicted) / (y_actual + epsilon))) * 100\n",
        "    return mape\n",
        "\n",
        "# 3. Calculate all metrics\n",
        "nn_mse = mean_squared_error(y_test, nn_prediction)\n",
        "nn_rmse = nn_mse ** 0.5\n",
        "nn_mae = mean_absolute_error(y_test, nn_prediction)\n",
        "nn_r2 = r2_score(y_test, nn_prediction)\n",
        "nn_mape = MAPE(y_test, nn_prediction)\n",
        "\n",
        "# 4. Print the final results\n",
        "print(\"\\n--- FINAL NEURAL NETWORK TEST PERFORMANCE (Tuned & Enhanced Features) ---\")\n",
        "print(f'Mean Squared Error (MSE): {nn_mse:.2f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {nn_rmse:.2f} minutes')\n",
        "print(f'Mean Absolute Error (MAE): {nn_mae:.2f} minutes')\n",
        "print(f'R-squared (R2 Score): {nn_r2:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {nn_mape:.2f}%')\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M9H4DXOlc4k",
        "outputId": "27553c44-1a6c-4664-a844-28c27875ebea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2871260639.py:35: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['store_avg_delivery_time'].fillna(global_mean, inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retraining final tuned model...\n",
            "Epoch 1/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 775.4174 - mae: 14.8683 - val_loss: 295.0294 - val_mae: 12.0875\n",
            "Epoch 2/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 639.9382 - mae: 12.3653 - val_loss: 296.6396 - val_mae: 12.2574\n",
            "Epoch 3/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 753.4326 - mae: 12.2508 - val_loss: 291.3465 - val_mae: 11.9785\n",
            "Epoch 4/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2427.7144 - mae: 13.1940 - val_loss: 294.0204 - val_mae: 12.0251\n",
            "Epoch 5/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 673.4949 - mae: 12.1845 - val_loss: 291.8960 - val_mae: 11.8717\n",
            "Epoch 6/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 723.9861 - mae: 12.1498 - val_loss: 294.7389 - val_mae: 12.3122\n",
            "Epoch 7/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 579.9418 - mae: 12.0011 - val_loss: 289.5601 - val_mae: 11.6977\n",
            "Epoch 8/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1402.5803 - mae: 12.3579 - val_loss: 286.5694 - val_mae: 11.8483\n",
            "Epoch 9/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 441.4643 - mae: 11.9059 - val_loss: 294.5495 - val_mae: 11.7465\n",
            "Epoch 10/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 814.9192 - mae: 12.0653 - val_loss: 287.6504 - val_mae: 12.0013\n",
            "Epoch 11/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 673.7466 - mae: 12.0216 - val_loss: 293.7463 - val_mae: 11.8116\n",
            "Epoch 12/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 1200.5161 - mae: 12.5219 - val_loss: 287.4991 - val_mae: 11.9718\n",
            "Epoch 13/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1060.1987 - mae: 12.1856 - val_loss: 285.6283 - val_mae: 11.8112\n",
            "Epoch 14/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 763.2775 - mae: 12.0402 - val_loss: 286.1719 - val_mae: 11.7472\n",
            "Epoch 15/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 347.3170 - mae: 11.8120 - val_loss: 315.3500 - val_mae: 12.2037\n",
            "Epoch 16/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1348.0864 - mae: 12.4612 - val_loss: 291.0340 - val_mae: 12.2397\n",
            "Epoch 17/100\n",
            "\u001b[1m882/882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 422.8609 - mae: 11.8713 - val_loss: 312.3510 - val_mae: 12.2945\n",
            "\u001b[1m1102/1102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\n",
            "--- FINAL NEURAL NETWORK TEST PERFORMANCE (Tuned & Enhanced Features) ---\n",
            "Mean Squared Error (MSE): 299.14\n",
            "Root Mean Squared Error (RMSE): 17.30 minutes\n",
            "Mean Absolute Error (MAE): 11.71 minutes\n",
            "R-squared (R2 Score): 0.1955\n",
            "Mean Absolute Percentage Error (MAPE): 26.22%\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model.save('delivery_time_model.h5')\n",
        "#final_model.save('delivery_time_model', save_format='tf')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-qPYtjDldpL",
        "outputId": "91ff668e-c03f-40e4-8fa0-1d7b2a85a841"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "joblib.dump(scaler, 'fitted_scaler.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoB4ptKfmmMV",
        "outputId": "e673a180-4a64-4509-aaba-dcbaaf075730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fitted_scaler.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "\n",
        "# Placeholder for the artifacts and feature list (Replace with your actual loaded artifacts and list)\n",
        "# final_model = tf.keras.models.load_model('delivery_time_model.h5')\n",
        "# fitted_scaler = joblib.load('fitted_scaler.joblib')\n",
        "# FINAL_COLUMNS = ['market_id', 'subtotal', 'total_items', 'num_distinct_items', ...]\n",
        "\n",
        "def predict_delivery_time(raw_data: dict, model, scaler, feature_columns, store_categories) -> float:\n",
        "    \"\"\"\n",
        "    Takes raw order data, processes it, and returns the predicted delivery time.\n",
        "\n",
        "    Args:\n",
        "        raw_data (dict): Dictionary of raw features for one new order.\n",
        "        model: The loaded Keras model.\n",
        "        scaler: The loaded fitted MinMaxScaler.\n",
        "        feature_columns (list): The list of columns used during model training.\n",
        "        store_categories (list): List of unique store categories for OHE.\n",
        "\n",
        "    Returns:\n",
        "        float: The predicted delivery time in minutes.\n",
        "    \"\"\"\n",
        "    # 1. Convert raw data to DataFrame for processing\n",
        "    input_df = pd.DataFrame([raw_data])\n",
        "\n",
        "    # 2. Convert timestamp strings to datetime objects for feature engineering\n",
        "    # Assuming the API receives 'created_at' as an ISO format string\n",
        "    input_df['created_at'] = pd.to_datetime(input_df['created_at'])\n",
        "\n",
        "    # 3. Recreate Time-Based Features (hours, day, rush hours, weekend)\n",
        "    input_df['hours'] = input_df['created_at'].dt.hour\n",
        "    input_df['day'] = input_df['created_at'].dt.dayofweek\n",
        "    input_df['is_lunch_rush'] = input_df['hours'].apply(lambda x: 1 if 11 <= x <= 14 else 0)\n",
        "    input_df['is_dinner_rush'] = input_df['hours'].apply(lambda x: 1 if 17 <= x <= 21 else 0)\n",
        "    input_df['is_weekend'] = input_df['day'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "\n",
        "    # 4. Handle Historical Average Feature\n",
        "    # NOTE: In a real system, you would query a database for this value,\n",
        "    # but here we use a placeholder or the global mean used during training.\n",
        "    # We assume 'store_avg_delivery_time' is part of the raw_data or is retrieved.\n",
        "    if 'store_avg_delivery_time' not in input_df.columns:\n",
        "         # Fallback: use the training set's global mean if the store average isn't available\n",
        "         global_mean_from_training = 45.0 # REPLACE with your actual global mean\n",
        "         input_df['store_avg_delivery_time'] = global_mean_from_training\n",
        "\n",
        "    # 5. One-Hot Encoding (OHE)\n",
        "    # CRITICAL: We use 'reindex' to ensure all OHE columns are present and in the right order\n",
        "    input_df = pd.get_dummies(input_df, columns=['store_primary_category'])\n",
        "\n",
        "    # Create a DataFrame template for the final feature set, ensuring column order\n",
        "    # Note: 'market_id' and other base features must be dropped/selected correctly\n",
        "    final_features = input_df.reindex(columns=feature_columns, fill_value=0)\n",
        "\n",
        "    # 6. Scaling\n",
        "    # Scale the feature vector using the fitted training scaler\n",
        "    x_scaled = scaler.transform(final_features)\n",
        "\n",
        "    # 7. Prediction\n",
        "    prediction = model.predict(x_scaled)[0][0]\n",
        "\n",
        "    # Return the prediction in minutes\n",
        "    return float(prediction)"
      ],
      "metadata": {
        "id": "TEzeEEfPmvhA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import datetime\n",
        "\n",
        "# --- 1. Load Artifacts ---\n",
        "# Replace the placeholders below with the actual file paths\n",
        "try:\n",
        "    final_model = tf.keras.models.load_model('delivery_time_model.h5')\n",
        "    fitted_scaler = joblib.load('fitted_scaler.joblib')\n",
        "\n",
        "    # CRITICAL: This list must be derived from x.columns after all processing.\n",
        "    # It ensures the input DataFrame is in the exact order the model expects.\n",
        "    # Placeholder example: You MUST use your actual list.\n",
        "    FINAL_COLUMNS = [\n",
        "        'market_id', 'subtotal', 'total_items', 'num_distinct_items',\n",
        "        'min_item_price', 'max_item_price', 'total_onshift_partners',\n",
        "        'total_busy_partners', 'total_outstanding_orders', 'store_avg_delivery_time',\n",
        "        'hours', 'day', 'is_lunch_rush', 'is_dinner_rush', 'is_weekend',\n",
        "        # And all your OHE categories, e.g.:\n",
        "        'category_american', 'category_mexican', 'category_thai',\n",
        "        # ... all other category columns ...\n",
        "    ]\n",
        "    # List of unique categories used to generate the OHE columns\n",
        "    STORE_CATEGORIES = ['american', 'mexican', 'thai', ...]\n",
        "    GLOBAL_MEAN_TIME = 45.0 # Replace with your calculated global mean\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model artifacts: {e}\")\n",
        "    # In a production environment, you would exit here if artifacts fail to load.\n",
        "\n",
        "# --- 2. Initialize FastAPI ---\n",
        "app = FastAPI(\n",
        "    title=\"Delivery Time Prediction API\",\n",
        "    description=\"Provides real-time estimates for order delivery based on a tuned Neural Network.\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "791hOXGd_91U",
        "outputId": "4cd94513-8f5f-46c3-e50e-901ec4b07c88"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model artifacts: Could not locate function 'mse'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'keras.metrics', 'class_name': 'function', 'config': 'mse', 'registered_name': 'mse'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Pydantic model for robust input validation\n",
        "class OrderInput(BaseModel):\n",
        "    # Base features\n",
        "    market_id: float\n",
        "    created_at: str # Receive as a string (ISO format)\n",
        "    subtotal: int\n",
        "    total_items: int\n",
        "    num_distinct_items: int\n",
        "    min_item_price: int\n",
        "    max_item_price: int\n",
        "    total_onshift_partners: float\n",
        "    total_busy_partners: float\n",
        "    total_outstanding_orders: float\n",
        "    store_primary_category: str\n",
        "\n",
        "    # Custom feature needed for prediction\n",
        "    store_avg_delivery_time: float # This should be looked up from a DB by the calling service\n",
        "\n",
        "# Define the API endpoint\n",
        "@app.post(\"/predict_time\")\n",
        "def predict(order_data: OrderInput):\n",
        "    # Convert the Pydantic model back to a simple dict for your pipeline function\n",
        "    raw_data = order_data.dict()\n",
        "\n",
        "    try:\n",
        "        # Call the core pipeline function\n",
        "        prediction = predict_delivery_time(\n",
        "            raw_data=raw_data,\n",
        "            model=final_model,\n",
        "            scaler=fitted_scaler,\n",
        "            feature_columns=FINAL_COLUMNS,\n",
        "            store_categories=STORE_CATEGORIES\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"estimated_delivery_minutes\": round(prediction, 2)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log the error (e.g., to a monitoring service)\n",
        "        return {\"status\": \"error\", \"message\": f\"Prediction failed due to: {str(e)}\"}\n",
        "\n",
        "# The raw predict_delivery_time function you defined needs to be in this same file or imported.\n",
        "# It should be runnable with the actual artifacts and feature lists defined above."
      ],
      "metadata": {
        "id": "OaU55vp0AJUH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uvicorn api:app --reload"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "cfrHhOqMANVc",
        "outputId": "3ef80559-a863-4a69-9807-c07da9a3d941"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3817926223.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3817926223.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    uvicorn api:app --reload\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkX9s5g-APXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}